{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66fa19b6-745b-4eec-ac8c-51268fce917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf0865-ad88-4f2c-a9a1-8b27525699f7",
   "metadata": {},
   "source": [
    "# 1. Initializing SparkSession\n",
    "The first step in any PySpark application is to create a SparkSession, which is the entry point to programming Spark with the Dataset and DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f864d8-2237-4b3a-b9e8-3e61bf0ab458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31219ee-b7dc-4039-b2ce-909eff4a2acb",
   "metadata": {},
   "source": [
    "This code initializes a SparkSession with a single thread (local[1]) and names the application as \"SparkByExamples.com\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a82236-66fe-4dca-9b9c-aaa166c0cad8",
   "metadata": {},
   "source": [
    "# 2. Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48309777-95a4-4891-b4ab-7578ea8f01c8",
   "metadata": {},
   "source": [
    "Using parallelize():\n",
    "This method creates an RDD from a list object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c833d01b-534a-4ca1-8e9b-4429353b68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from parallelize\n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e5d820-f944-4d9a-8936-8cbbd742921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the content of your file\n",
    "text_content = \"\"\"Hello, this is a test file.\n",
    "This file is used for creating an RDD.\n",
    "Each line will be an element in the RDD.\"\"\"\n",
    "\n",
    "# Specify the path and name of your file\n",
    "file_path = \"textFile.txt\"\n",
    "\n",
    "# Write the content to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(text_content)\n",
    "\n",
    "# Now you can use this file path in your SparkContext.textFile() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f178c453-8a3a-4978-9cb8-3a87e2b41fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from external Data source\n",
    "# Ensure you replace \"/path/textFile.txt\" with the actual path to the file you want to use\n",
    "rdd2 = spark.sparkContext.textFile(\"textFile.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639aac0-4b75-49b8-9f9c-db1786a36a8e",
   "metadata": {},
   "source": [
    "# 3. RDD Operations\n",
    "Transformations and Actions:\r\n",
    "Transformations create a new RDD from an existing one. Examples include map, filter, flatMap, etc.\r\n",
    "Actions return a value after computing a result from an RDD. Examples include count, first, collect, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9847f768-1e5e-4e8a-b640-9842766d3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Example of transformation and action\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5]) \n",
    "rddFiltered = rdd.filter(lambda x: x%2 == 0) # Transformation \n",
    "print(rddFiltered.collect()) # Action, outputs: [2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04f1f4c7-1a3b-40a8-87e1-f9b35e53a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Another example in this cell\n",
    "# Create a list of data\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create an RDD from the list\n",
    "distData = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Print the RDD\n",
    "print(distData.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d4104-d61b-4c75-a66d-ee84d24c368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "189d7c64-d761-455c-a4cd-1ad5263ecd5c",
   "metadata": {},
   "source": [
    "RDD Persistence:\n",
    "Using cache() or persist() methods, you can store RDDs in memory for faster access in subsequent actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53ceb053-c948-4a0c-a331-7417713f7056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[20] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache() # Cache the RDD in memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfef14-4292-4f5b-b1f6-b8d5172e8589",
   "metadata": {},
   "source": [
    "# 4. Converting RDDs to DataFrames and vice-versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21daf0fe-464b-48fe-bdc6-d0e36487c86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert RDD to DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dfFromRDD \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumbers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[1;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1007\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m   1005\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1673\u001b[0m     )\n\u001b[0;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
     ]
    }
   ],
   "source": [
    "# Convert RDD to DataFrame\n",
    "dfFromRDD = rdd.toDF([\"numbers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a709f784-9279-4b29-925d-9fb2d613f839",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfFromRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert DataFrame to RDD\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rddFromDF \u001b[38;5;241m=\u001b[39m \u001b[43mdfFromRDD\u001b[49m\u001b[38;5;241m.\u001b[39mrdd\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfFromRDD' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to RDD\n",
    "rddFromDF = dfFromRDD.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e5b8101-5ed1-43ce-aedf-6d5cf066c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Assuming 'rdd' is your existing RDD of integers\n",
    "# Map each integer in the RDD to a tuple\n",
    "rdd_tuples = rdd.map(lambda x: (x,))\n",
    "\n",
    "# Now convert to DataFrame\n",
    "dfFromRDD = rdd_tuples.toDF([\"numbers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "691a7ba3-af81-49a3-b440-2d616649acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Map each integer to a Row object with a 'numbers' field\n",
    "rdd_rows = rdd.map(lambda x: Row(numbers=x))\n",
    "\n",
    "# Convert to DataFrame\n",
    "dfFromRDD = spark.createDataFrame(rdd_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efb07a-432c-4034-9bf3-9f06231996e0",
   "metadata": {},
   "source": [
    "# Basic DataFrame Operations\n",
    "- **Show DataFrame**\r\n",
    "To display the contents of the DataFrame, you can use the .show() method, which prints the DataFrame rows in a tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d3f6bc-ecdd-4f89-aa3f-183ad911273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90527e-25f4-44dc-b6a7-afce448a887a",
   "metadata": {},
   "source": [
    "### Print Schema\n",
    "The .printSchema() method displays the schema of the DataFrame, showing the column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9be9606-0efe-46ac-9369-a5d8ae36ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493a831-6ce9-46e7-b4b0-90b290ccb65d",
   "metadata": {},
   "source": [
    "# DataFrame Transformations\n",
    "Transformations on DataFrames return a new DataFrame and are lazily evaluated, similar to RDD transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405971-e8b6-4914-9ba4-f941b9656df7",
   "metadata": {},
   "source": [
    "- **Select Columns**\n",
    "You can select specific columns from a DataFrame using the .select() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79a8d4f8-33c0-4b31-a7c0-1daf4427304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSelected = dfFromRDD.select(\"numbers\")\n",
    "dfSelected.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3c8db-8e8e-4304-8fb8-0ef87028d1e1",
   "metadata": {},
   "source": [
    "- **Filter Rows**\n",
    "Filter rows based on a condition using the .filter() or .where() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbbc8a4b-aaf0-4b2e-a26c-393f4d632238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFiltered = dfFromRDD.filter(dfFromRDD.numbers > 3)  # or .where(dfFromRDD.numbers > 3)\n",
    "dfFiltered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04db856-c741-4c66-91a6-4254f83a9d50",
   "metadata": {},
   "source": [
    "- **Add New Column**\n",
    "Add a new column to the DataFrame using the .withColumn() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c5355d9-ec0e-4e08-9692-83ee3d1f1b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|numbers|numbers_times_two|\n",
      "+-------+-----------------+\n",
      "|      1|                2|\n",
      "|      2|                4|\n",
      "|      3|                6|\n",
      "|      4|                8|\n",
      "|      5|               10|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfWithNewColumn = dfFromRDD.withColumn(\"numbers_times_two\", col(\"numbers\") * 2)\n",
    "dfWithNewColumn.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a917f-fbac-4854-bf97-414a14968de7",
   "metadata": {},
   "source": [
    "# DataFrame Actions \n",
    "Actions on a DataFrame trigger the execution of the computation and return results to the driver or write to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3de9d-cd04-4169-976e-076ad2f1bdcd",
   "metadata": {},
   "source": [
    "- **Collect**\n",
    "Collect the result rows to the driver as a list of Row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b482f9c9-394f-4c27-b108-f738e539be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(numbers=1), Row(numbers=2), Row(numbers=3), Row(numbers=4), Row(numbers=5)]\n"
     ]
    }
   ],
   "source": [
    "rows = dfFromRDD.collect()\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424f9bf-27fc-4a7e-817d-c3a92573ee0b",
   "metadata": {},
   "source": [
    "- **Count**\n",
    "Return the number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "949d8dcc-bf14-49cd-8611-6b7214d97eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "rowCount = dfFromRDD.count()\n",
    "print(rowCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976772d-0e23-470d-a239-b4ca0c22f835",
   "metadata": {},
   "source": [
    "- **Save to Files**\n",
    "Save the DataFrame to an external storage system, like a file in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56f4ad06-ab5e-4f97-93de-b52e8f02de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromRDD.write.csv(\"numbers.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194acc3-88f7-4849-81ce-7f35c5c8b818",
   "metadata": {},
   "source": [
    "# Joining DataFrames\n",
    "If you have another DataFrame and want to join it with dfFromRDD based on a common column, you can use the .join() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d39e1-f3de-4521-b050-83fa14836d66",
   "metadata": {},
   "source": [
    "#### Define dfOther DataFrame\n",
    "First, let's create a DataFrame named dfOther with an id column and another column of your choice. For simplicity, I'll create it from a list of tuples, each representing a row in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ae5f0ec-52d9-4f36-b432-5e0518283075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    A|\n",
      "|  2|    B|\n",
      "|  3|    C|\n",
      "|  4|    D|\n",
      "|  5|    E|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Assuming SparkSession is already initialized as 'spark'\n",
    "\n",
    "# Sample data for dfOther\n",
    "dataOther = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (4, \"D\"), (5, \"E\")]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "dfOther = spark.createDataFrame(dataOther, [\"id\", \"value\"])\n",
    "\n",
    "# Show the DataFrame to verify its creation\n",
    "dfOther.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab896fb8-be05-4e5d-b49a-1d1eaf06ca61",
   "metadata": {},
   "source": [
    "# Perform the Join Operation\n",
    "Now that dfOther is defined and has an id column, you can perform the join operation with dfFromRDD as intended. Make sure both DataFrames have matching column types for the join to work correctly. Assuming dfFromRDD has a numbers column that you want to join on the id column of dfOther:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17606a1a-6bbd-4333-b5b6-3562513e72e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|numbers| id|value|\n",
      "+-------+---+-----+\n",
      "|      1|  1|    A|\n",
      "|      2|  2|    B|\n",
      "|      3|  3|    C|\n",
      "|      4|  4|    D|\n",
      "|      5|  5|    E|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join operation\n",
    "dfJoined = dfFromRDD.join(dfOther, dfFromRDD.numbers == dfOther.id)\n",
    "\n",
    "# Show the result of the join\n",
    "dfJoined.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c4f87-3f2c-43ba-ada6-8c390516d83c",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions\n",
    "Grouping data by one or more columns and then applying an aggregate function is a common operation in data analysis. PySpark DataFrames support a variety of aggregate functions such as count(), sum(), avg(), max(), and min()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "483ff29a-68a5-4180-a8ec-a906e3783137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming dfFromRDD is your initial DataFrame and it has a column named 'numbers'\n",
    "dfFromRDD = dfFromRDD.withColumn(\"numbers_times_two\", col(\"numbers\") * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e57618f-80c5-4de8-b36b-7ccf4b3edd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|numbers|average_times_two|\n",
      "+-------+-----------------+\n",
      "|      5|             10.0|\n",
      "|      1|              2.0|\n",
      "|      3|              6.0|\n",
      "|      2|              4.0|\n",
      "|      4|              8.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Now perform the grouping and aggregation\n",
    "dfGrouped = dfFromRDD.groupBy(\"numbers\").agg(F.avg(\"numbers_times_two\").alias(\"average_times_two\"))\n",
    "dfGrouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafe576-b105-481b-bda7-702b190aea2c",
   "metadata": {},
   "source": [
    "# Advanced Data Analysis and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3c62a-9cef-4a03-ba29-e01f1d6ce821",
   "metadata": {},
   "source": [
    "#### Using Window Functions for Advanced Analytics:\n",
    "\n",
    "- Window functions allow you to perform calculations across subsets of data without collapsing rows. You've seen a simple example, but they can be used for more complex scenarios like calculating moving averages or ranking rows within groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ce333-2551-4989-9d34-29a1615bc3a3",
   "metadata": {},
   "source": [
    "#### Example: Calculating Moving Averages and Ranking\n",
    "Assuming you have a DataFrame (dfFromRDD) with the following structure:\n",
    "\n",
    "- A **category** column to partition data,\n",
    "- A **timestamp** or similar column to order data,\n",
    "- A **value** column on which we want to perform calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b26cc1-4aad-44fc-949d-fa07db1c3564",
   "metadata": {},
   "source": [
    "Our goal is to calculate a moving average of value over a specified window and rank rows based on value within each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54196d0-2bd1-44ca-837d-aea5c5864a65",
   "metadata": {},
   "source": [
    "Step 1: Import Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20bad04e-ecf2-44e1-bce8-8d0f8176f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68ac72-0fad-44df-8902-b224ec26184b",
   "metadata": {},
   "source": [
    "Step 2: Define Window Specifications\n",
    "For Moving Average: Define a window partitioned by category, ordered by timestamp, and specify a range between the current row and 2 rows back for the moving average calculation.\n",
    "\n",
    "For Ranking: Define a window partitioned by category and ordered by value in descending order to rank the rows based on value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86e10275-3c3a-4781-9d72-cd807ec9f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window specification for moving average\n",
    "windowSpecAvg = Window.partitionBy(\"category\").orderBy(\"timestamp\").rowsBetween(-2, 0)\n",
    "\n",
    "# Window specification for ranking\n",
    "windowSpecRank = Window.partitionBy(\"category\").orderBy(F.desc(\"value\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c91fc-7a31-455f-9d97-d6adb74ddb0e",
   "metadata": {},
   "source": [
    "Step 3: Apply Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcfa36bc-1699-421c-860a-272f22a2ab00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`numbers`, `numbers_times_two`].;\n'Project [numbers#2L, numbers_times_two#150L, avg(numbers_times_two#150L) windowspecdefinition('category, 'timestamp ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS moving_avg#179]\n+- Project [numbers#2L, (numbers#2L * cast(2 as bigint)) AS numbers_times_two#150L]\n   +- LogicalRDD [numbers#2L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m windowSpecAvg \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrowsBetween(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate moving average for 'numbers_times_two'\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dfWithMovingAvg \u001b[38;5;241m=\u001b[39m \u001b[43mdfFromRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoving_avg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumbers_times_two\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindowSpecAvg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m dfWithMovingAvg\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5169\u001b[0m     )\n\u001b[1;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`numbers`, `numbers_times_two`].;\n'Project [numbers#2L, numbers_times_two#150L, avg(numbers_times_two#150L) windowspecdefinition('category, 'timestamp ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS moving_avg#179]\n+- Project [numbers#2L, (numbers#2L * cast(2 as bigint)) AS numbers_times_two#150L]\n   +- LogicalRDD [numbers#2L], false\n"
     ]
    }
   ],
   "source": [
    "# Ensure the window specification is correct\n",
    "# Assuming 'category' and 'timestamp' columns exist. If not, replace these with your actual partitioning and ordering columns\n",
    "windowSpecAvg = Window.partitionBy(\"category\").orderBy(\"timestamp\").rowsBetween(-2, 0)\n",
    "\n",
    "# Calculate moving average for 'numbers_times_two'\n",
    "dfWithMovingAvg = dfFromRDD.withColumn(\"moving_avg\", F.avg(\"numbers_times_two\").over(windowSpecAvg))\n",
    "\n",
    "dfWithMovingAvg.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f0fab-c178-40ce-b284-a572b5007e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c529634-fb72-4d2e-a573-d149f795f28a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`numbers`, `numbers_times_two`].;\n'Project [numbers#2L, numbers_times_two#150L, avg('value) windowspecdefinition('category, 'timestamp ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS moving_avg#181]\n+- Project [numbers#2L, (numbers#2L * cast(2 as bigint)) AS numbers_times_two#150L]\n   +- LogicalRDD [numbers#2L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate moving average\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dfWithMovingAvg \u001b[38;5;241m=\u001b[39m \u001b[43mdfFromRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoving_avg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindowSpecAvg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\sql\\dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5169\u001b[0m     )\n\u001b[1;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\demo\\spark-3.5.0\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`numbers`, `numbers_times_two`].;\n'Project [numbers#2L, numbers_times_two#150L, avg('value) windowspecdefinition('category, 'timestamp ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS moving_avg#181]\n+- Project [numbers#2L, (numbers#2L * cast(2 as bigint)) AS numbers_times_two#150L]\n   +- LogicalRDD [numbers#2L], false\n"
     ]
    }
   ],
   "source": [
    "# Calculate moving average\n",
    "dfWithMovingAvg = dfFromRDD.withColumn(\"moving_avg\", F.avg(\"value\").over(windowSpecAvg)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01f11d50-7387-4e94-be79-df214b9517bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfWithMovingAvg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rank rows within each category based on value\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dfWithRank \u001b[38;5;241m=\u001b[39m \u001b[43mdfWithMovingAvg\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrank()\u001b[38;5;241m.\u001b[39mover(windowSpecRank))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfWithMovingAvg' is not defined"
     ]
    }
   ],
   "source": [
    "# Rank rows within each category based on value\n",
    "dfWithRank = dfWithMovingAvg.withColumn(\"rank\", F.rank().over(windowSpecRank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9252ab53-9115-4852-8a0d-989105d429d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+\n",
      "|numbers|numbers_times_two|cumulative_sum|\n",
      "+-------+-----------------+--------------+\n",
      "|      1|                2|             1|\n",
      "|      2|                4|             3|\n",
      "|      3|                6|             6|\n",
      "|      4|                8|            10|\n",
      "|      5|               10|            15|\n",
      "+-------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window specification for cumulative sum (assuming ordering by 'numbers' for demonstration)\n",
    "windowSpecCumSum = Window.orderBy(\"numbers\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate cumulative sum for 'numbers'\n",
    "dfWithCumSum = dfFromRDD.withColumn(\"cumulative_sum\", F.sum(\"numbers\").over(windowSpecCumSum))\n",
    "\n",
    "dfWithCumSum.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265e4e2-11a1-43b0-8590-1589ec34d32f",
   "metadata": {},
   "source": [
    "# 1. Advanced Window Function: Calculating Group-Specific Moving Averages\n",
    "Let's simulate a scenario where your DataFrame includes sales data with multiple stores (categories), dates, and sales amounts. The goal is to calculate a 7-day moving average of sales for each store.\n",
    "\n",
    "Note: This example is hypothetical. Adjust your DataFrame creation or modification steps according to your actual data structure.\n",
    "\n",
    "Sample Data Preparation\n",
    "First, let's create a DataFrame representing our sales data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2244c0b-1609-40b4-9f47-a53811a9ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|store|      date|sales|\n",
      "+-----+----------+-----+\n",
      "|    A|2020-01-01|  100|\n",
      "|    A|2020-01-02|  150|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "# Example sales data: store, date, sales\n",
    "sales_data = [\n",
    "    Row(store=\"A\", date=datetime.date(2020, 1, 1), sales=100),\n",
    "    Row(store=\"A\", date=datetime.date(2020, 1, 2), sales=150),\n",
    "    # Add more rows for each store and date\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "dfSales = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Assuming dfSales has 'store', 'date', and 'sales' columns\n",
    "dfSales.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504b788-5dd4-437a-9102-63722f34aec0",
   "metadata": {},
   "source": [
    "Calculating Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af0036a6-d081-4774-8838-a1426a25a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+---------------+\n",
      "|store|      date|sales|7_day_avg_sales|\n",
      "+-----+----------+-----+---------------+\n",
      "|    A|2020-01-01|  100|          100.0|\n",
      "|    A|2020-01-02|  150|          125.0|\n",
      "+-----+----------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define the window specification\n",
    "windowSpec = Window.partitionBy(\"store\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Calculate 7-day moving average of sales for each store\n",
    "dfMovingAvg = dfSales.withColumn(\"7_day_avg_sales\", avg(\"sales\").over(windowSpec))\n",
    "\n",
    "dfMovingAvg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019ef5a-c030-46e7-b6b4-b440d6eedb83",
   "metadata": {},
   "source": [
    "# 2. Ranking Data Within Groups\n",
    "Using the same sales DataFrame, we might want to rank stores by their sales on each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "794e6f71-6394-498a-9216-39e13aeb5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+----+\n",
      "|store|      date|sales|rank|\n",
      "+-----+----------+-----+----+\n",
      "|    A|2020-01-01|  100|   1|\n",
      "|    A|2020-01-02|  150|   1|\n",
      "+-----+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Define window spec for ranking\n",
    "windowSpecRank = Window.partitionBy(\"date\").orderBy(dfSales.sales.desc())\n",
    "\n",
    "# Rank stores by sales within each date\n",
    "dfRanked = dfSales.withColumn(\"rank\", rank().over(windowSpecRank))\n",
    "\n",
    "dfRanked.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c642354-a77f-4ffe-8499-317fe5015b54",
   "metadata": {},
   "source": [
    "# 3. Integration with Machine Learning\n",
    "After performing data preprocessing and feature engineering using window functions, you can use PySpark MLlib to build predictive models. Let's say you want to predict future sales based on historical data.\n",
    "\n",
    "Feature Engineering\n",
    "Assume dfSales now includes a 7_day_avg_sales column from our previous step. You might want to create feature vectors from this and other relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee343ec1-e2a3-4134-a08d-3907d1712774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045817de-7423-4d06-a13d-636ab89a1fc1",
   "metadata": {},
   "source": [
    "## Step 1: Calculate the 7_day_avg_sales Column\n",
    "Ensure you have the calculation for 7_day_avg_sales correctly applied to dfSales. For example, if calculating a moving average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef7c92da-34bc-4e2f-a275-18c88bef0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the window specification for moving average calculation\n",
    "windowSpec = Window.partitionBy(\"store\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Calculate the moving average and add as a new column\n",
    "dfSales = dfSales.withColumn(\"7_day_avg_sales\", F.avg(\"sales\").over(windowSpec)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ebe64-697f-408a-94fe-757c5bea3d09",
   "metadata": {},
   "source": [
    "## Step 2: Use the VectorAssembler\n",
    "After confirming that dfSales now includes the 7_day_avg_sales column, proceed with the VectorAssembler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7d64571-356c-4b43-8c8a-0ef94c808b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assuming dfSales has been correctly prepared with a '7_day_avg_sales' column\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"7_day_avg_sales\"], outputCol=\"features\")\n",
    "dfFeatures = assembler.transform(dfSales)\n",
    "\n",
    "# Correct way to select features and label\n",
    "dfModelData = dfFeatures.select(\"features\", dfFeatures.sales.alias(\"label\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977e532-3d64-4e4b-9b9a-a570e8386c9b",
   "metadata": {},
   "source": [
    "## Step 4: Train a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57457d-0768-4d76-a7c5-0a9c95180211",
   "metadata": {},
   "source": [
    "We will use a simple Linear Regression model as an example. This model will try to predict the sales (label) based on the 7_day_avg_sales (features) you've prepared.\n",
    "\n",
    "- **4.1 Split the Data**\n",
    "\n",
    "Split the data into training and test datasets to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5727f9fa-6b5e-4390-97ec-760e9d0003e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(trainingData, testData) = dfModelData.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b0b9867e-1e72-47e4-aa36-27351996b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model\n",
    "lrModel = lr.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8150-c04d-4574-9931-c911c5e219b0",
   "metadata": {},
   "source": [
    "## Step 5: Make Predictions and Evaluate the Model\n",
    "Use the trained model to make predictions on the test data and evaluate the model's performance.\n",
    "\n",
    "- **5.1 Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f5b639f-c6f6-4170-9363-428c4ab69f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|prediction|label|features|\n",
      "+----------+-----+--------+\n",
      "|     150.0|  100| [100.0]|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f011372-6d47-4e4c-a351-fcd545fb0dab",
   "metadata": {},
   "source": [
    "- **5.2 Evaluate the Model**\n",
    "Evaluate the model using an evaluator, such as RMSE (Root Mean Squared Error), to quantify the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c31a2a3-39e1-45d0-bc03-f4dd42e8f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 50\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a0b66-478e-4411-b2ae-d80e4fedd2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
